## creating a job ##

parameters := split parameters into array or arrays [['param','value'],['param2','value2'],...]
queries := split query string into array of queries ['fasta-entry','fasta-entry2',...]
try
    job_uid := SQL: SELECT * FROM create_job(program, targetdb_name, parameters, queries);
    return job_uid
catch error
    output error message (will be thrown by create_job on invalid parameters etc.)




## a simple worker ##

max_forks := maximum number or processes running on this machine
hostname := unique hostname (or any other unique string) for this machine 
supported_programs := array of programs installed on this worker (e.g. ['blastn', 'blastp', 'blastx', 'tblastp', 'tblastx'] )

while true do:
    job := SQL: SELECT * FROM request_job(max_forks, hostname, supported_programs)
    if rowcount_returned == 1 then
        do_fork
        if we are child then
            register repeating function: call SQL: keepalive_ping(job['job_query_id']) regularly. return value of this call determines the maximum time in seconds until we have to call again if -1 is returned, this job has been cancelled

            SQL: report_job_pid(job['job_query_id'], mypid)
            if local database with name job['target_db'] does not exist or does not have md5 job['target_db_md5'] then
                download job['target_db_download_uri'], check md5 and unzip it
            endif
            replace $DBFILE in job['parameters'] with path to DBFILE
            (returncode, stdout, stderr) := exec: job['programname'] job['parameters']
            SQL: report_job_result(job['job_query_id'], returncode, stdout, stderr)
            exit;
        else if we are parent then
            continue
        endif
    else
        sleep a few seconds
    endif
done